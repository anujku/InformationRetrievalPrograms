Implement your own crawler
Start crawling with the same seed: www.ccs.neu.edu
You may use existing libraries to request documents over HTTP.
You may use existing libraries to parse the content of HTML pages.
Implement your own code to keep track of what you crawled and decide what to crawl next.
Extract links from HTML pages. Note that pages of type text/html will not necessarily have URLs that end in .html.
Record both HTML and PDF pages. PDF pages will be dead ends (sinks). Ignore other document types.
Repect robots.txt. For ccs.neu.edu, it looks like:
User-agent: htdig
Disallow: /tools/checkbot/
Disallow: /home/ftp/
Disallow: /home/www/

User-agent: *
Disallow: /tools/checkbot/
Disallow: /tools/hypermail/dox/
Disallow: /home/ftp/
Disallow: /home/www/
Disallow: /home/sxhan/com1105/
Disallow: /course/com1390/roster.pdf 
Disallow: /home/yimin/grades.html
Disallow: /home/fceria01/
Use a five-second delay between requests.
Crawl until you have 100 unique links. You do not need to keep the page contents.
Compare to the results you obtained from wget. What differences in crawling strategy can you see?
